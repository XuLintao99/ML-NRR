#DNN classification main code:
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras import regularizers
from tensorflow.keras.optimizers import Adam

#Standardization:
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

#converting the original categorical vector into one-hot encoding:
y_cla=to_categorical(y, num_classes=2)


#DNN:
model = Sequential()
model.add(Dense(10, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(2, activation='softmax'))
from tensorflow.keras.optimizers.schedules import ExponentialDecay
initial_learning_rate = 0.0045
decay_steps = 10
decay_rate = 0.7
lr_schedule = ExponentialDecay(
    initial_learning_rate,
    decay_steps=decay_steps,
    decay_rate=decay_rate,
    staircase=True)
adam_optimizer = Adam(learning_rate=lr_schedule, beta_1=0.85, beta_2=0.999, epsilon=1e-09,weight_decay=0.00015)
model.compile(loss='binary_crossentropy',
              optimizer=adam_optimizer,
             metrics=['accuracy'])
history=model.fit(x_train, y_train, epochs=200, batch_size=150, validation_split=0.15)
plt.plot(history.history['loss'], label='Training loss',zorder=2)
plt.plot(history.history['val_loss'],label='Validation loss')
plt.ylabel('Loss',fontname='Times New Roman',fontsize=15)
plt.xlabel('Epoch',fontname='Times New Roman',fontsize=15)
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()


#Some of the hyperparameters used for XGBoost regressor model are as follows:
colsample_bylevel=0.4
colsample_bytree=0.6
learning_rate=0.15
max_depth=4
min_child_weight=1
n_estimators=100
alpha=0.45
subsample=0.27
